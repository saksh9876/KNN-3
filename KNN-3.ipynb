{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f78b2d6d-690f-4fc4-b72c-24c2f1f0b1dc",
   "metadata": {},
   "source": [
    "Q1. Write a Python code to implement the KNN classifier algorithm on load_iris dataset in\n",
    "sklearn.datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1627f1df-76f4-4d3c-8eb2-9dbfdc8109fb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "ANS:\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    Certainly! Here's a Python code example that demonstrates how to implement the K-Nearest Neighbors (KNN) classifier algorithm on the `load_iris` dataset from `sklearn.datasets`:\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # Features\n",
    "y = iris.target  # Target labels\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a KNN classifier with a specified number of neighbors (k)\n",
    "k = 3\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = knn_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy on the test set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "```\n",
    "\n",
    "In this code:\n",
    "\n",
    "1. We import the necessary libraries, including `load_iris` to load the dataset, `train_test_split` to split it into training and testing sets, `KNeighborsClassifier` to create the KNN classifier, and `accuracy_score` to evaluate the model's performance.\n",
    "\n",
    "2. We load the Iris dataset and split it into features (X) and target labels (y).\n",
    "\n",
    "3. We split the dataset into training and testing sets using `train_test_split`. In this example, 70% of the data is used for training, and 30% is used for testing.\n",
    "\n",
    "4. We create a KNN classifier with `KNeighborsClassifier` and specify the number of neighbors (k) to use. In this case, we set `k` to 3.\n",
    "\n",
    "5. We fit the KNN classifier to the training data using the `fit` method.\n",
    "\n",
    "6. We make predictions on the test data using the `predict` method.\n",
    "\n",
    "7. Finally, we calculate and print the accuracy of the KNN classifier on the test set to evaluate its performance.\n",
    "\n",
    "You can modify the value of `k` or other hyperparameters and explore different evaluation metrics based on your specific requirements and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0db59a-ca94-47de-9ca7-f5b1a4a4ced7",
   "metadata": {},
   "source": [
    "Q2. Write a Python code to implement the KNN regressor algorithm on load_boston dataset in\n",
    "sklearn.datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08853c45-9907-46f0-8245-9e3fc7dd2b76",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ANS:\n",
    "    \n",
    "    \n",
    "    \n",
    "    Certainly! Here's a Python code example that demonstrates how to implement the K-Nearest Neighbors (KNN) regressor algorithm on the `load_boston` dataset from `sklearn.datasets`:\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Load the Boston Housing dataset\n",
    "boston = load_boston()\n",
    "X = boston.data  # Features\n",
    "y = boston.target  # Target (housing prices)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a KNN regressor with a specified number of neighbors (k)\n",
    "k = 3\n",
    "knn_regressor = KNeighborsRegressor(n_neighbors=k)\n",
    "\n",
    "# Fit the regressor to the training data\n",
    "knn_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = knn_regressor.predict(X_test)\n",
    "\n",
    "# Calculate and print evaluation metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"R-squared (R2) Score: {r2:.2f}\")\n",
    "```\n",
    "\n",
    "In this code:\n",
    "\n",
    "1. We import the necessary libraries, including `load_boston` to load the dataset, `train_test_split` to split it into training and testing sets, `KNeighborsRegressor` to create the KNN regressor, and evaluation metrics (`mean_squared_error` and `r2_score`) to assess the model's performance.\n",
    "\n",
    "2. We load the Boston Housing dataset and split it into features (X) and target values (y), where the target represents housing prices.\n",
    "\n",
    "3. We split the dataset into training and testing sets using `train_test_split`. In this example, 70% of the data is used for training, and 30% is used for testing.\n",
    "\n",
    "4. We create a KNN regressor with `KNeighborsRegressor` and specify the number of neighbors (k) to use. In this case, we set `k` to 3.\n",
    "\n",
    "5. We fit the KNN regressor to the training data using the `fit` method.\n",
    "\n",
    "6. We make predictions on the test data using the `predict` method.\n",
    "\n",
    "7. Finally, we calculate and print evaluation metrics, including the Mean Squared Error (MSE) and R-squared (R2) score, to assess the performance of the KNN regressor.\n",
    "\n",
    "You can adjust the value of `k` or other hyperparameters and explore different evaluation metrics based on your specific regression task and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f9af6e-869c-49be-bb8b-d2903290b2be",
   "metadata": {},
   "source": [
    "Q3. Write a Python code snippet to find the optimal value of K for the KNN classifier algorithm using\n",
    "cross-validation on load_iris dataset in sklearn.datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07659f03-a63e-4813-a348-483087287fc0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "ANS:\n",
    "    \n",
    "    \n",
    "    \n",
    "    To find the optimal value of K for the K-Nearest Neighbors (KNN) classifier using cross-validation on the `load_iris` dataset from `sklearn.datasets`, you can perform a grid search over a range of K values and evaluate the model's performance using cross-validation. Here's a Python code snippet to achieve this:\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # Features\n",
    "y = iris.target  # Target labels\n",
    "\n",
    "# Split the dataset into training and testing sets (not used for cross-validation)\n",
    "X_train, _, y_train, _ = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define a range of K values to test\n",
    "k_values = list(range(1, 21))  # You can adjust the range as needed\n",
    "\n",
    "# Create an empty dictionary to store K-values and their corresponding cross-validation scores\n",
    "k_scores = {}\n",
    "\n",
    "# Perform cross-validation for each K value\n",
    "for k in k_values:\n",
    "    knn_classifier = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn_classifier, X_train, y_train, cv=5, scoring='accuracy')  # 5-fold cross-validation\n",
    "    k_scores[k] = np.mean(scores)\n",
    "\n",
    "# Find the K value with the highest cross-validation score\n",
    "best_k = max(k_scores, key=k_scores.get)\n",
    "best_score = k_scores[best_k]\n",
    "\n",
    "print(f\"Best K: {best_k}\")\n",
    "print(f\"Best Cross-Validation Score: {best_score:.4f}\")\n",
    "```\n",
    "\n",
    "In this code:\n",
    "\n",
    "1. We load the Iris dataset and split it into features (X) and target labels (y).\n",
    "\n",
    "2. We split the dataset into training and testing sets (not used for cross-validation) using `train_test_split`. You can adjust the test size as needed.\n",
    "\n",
    "3. We define a range of K values (from 1 to 20 in this example) that we want to test.\n",
    "\n",
    "4. We create an empty dictionary `k_scores` to store the K-values and their corresponding cross-validation scores.\n",
    "\n",
    "5. We loop through each K value, create a KNN classifier with that K value, and perform 5-fold cross-validation (`cv=5`) to compute the mean accuracy score for each K value.\n",
    "\n",
    "6. We record the K-value and its cross-validation score in the `k_scores` dictionary.\n",
    "\n",
    "7. Finally, we identify the K value with the highest cross-validation score as the optimal K for the KNN classifier.\n",
    "\n",
    "You can adjust the range of K values, the number of folds in cross-validation, and the scoring metric to suit your specific needs. The code will help you find the optimal K value for your KNN classifier on the `load_iris` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5069b20f-094f-4106-977d-7db1d6ebe67a",
   "metadata": {},
   "source": [
    "Q4. Implement the KNN regressor algorithm with feature scaling on load_boston dataset in\n",
    "sklearn.datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb532ff1-36ea-44ed-aab7-69c2bb226435",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "ANS:\n",
    "    \n",
    "    \n",
    "    \n",
    "    To implement the K-Nearest Neighbors (KNN) regressor algorithm with feature scaling on the `load_boston` dataset from `sklearn.datasets`, you can follow these steps. We'll use the `StandardScaler` from scikit-learn for feature scaling:\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the Boston Housing dataset\n",
    "boston = load_boston()\n",
    "X = boston.data  # Features\n",
    "y = boston.target  # Target (housing prices)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Perform feature scaling using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create a KNN regressor with a specified number of neighbors (k)\n",
    "k = 3\n",
    "knn_regressor = KNeighborsRegressor(n_neighbors=k)\n",
    "\n",
    "# Fit the regressor to the scaled training data\n",
    "knn_regressor.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the scaled test data\n",
    "y_pred = knn_regressor.predict(X_test_scaled)\n",
    "\n",
    "# Calculate and print evaluation metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"R-squared (R2) Score: {r2:.2f}\")\n",
    "```\n",
    "\n",
    "In this code:\n",
    "\n",
    "1. We load the Boston Housing dataset and split it into features (X) and target values (y), representing housing prices.\n",
    "\n",
    "2. We split the dataset into training and testing sets using `train_test_split`, with 70% of the data for training and 30% for testing.\n",
    "\n",
    "3. We perform feature scaling using `StandardScaler`. This step standardizes the features to have zero mean and unit variance, which is important for KNN because it relies on distance metrics.\n",
    "\n",
    "4. We create a KNN regressor with `KNeighborsRegressor` and specify the number of neighbors (k) to use. In this case, we set `k` to 3.\n",
    "\n",
    "5. We fit the KNN regressor to the scaled training data using the `fit` method.\n",
    "\n",
    "6. We make predictions on the scaled test data using the `predict` method.\n",
    "\n",
    "7. Finally, we calculate and print evaluation metrics, including the Mean Squared Error (MSE) and R-squared (R2) score, to assess the performance of the KNN regressor.\n",
    "\n",
    "By applying feature scaling, you ensure that all features have the same scale, which is a crucial step for KNN and helps improve the performance of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295372f9-0922-4343-9e21-684cc826c1d7",
   "metadata": {},
   "source": [
    "Q5. Write a Python code snippet to implement the KNN classifier algorithm with weighted voting on\n",
    "load_iris dataset in sklearn.datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f31b471-315f-400a-aaa7-6455698b3038",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "ANS:\n",
    "    \n",
    "    \n",
    "    \n",
    "   Certainly! You can implement the K-Nearest Neighbors (KNN) classifier algorithm with weighted voting on the `load_iris` dataset from `sklearn.datasets`. In weighted voting, the contributions of neighbors to the decision are weighted by their distance. Here's a Python code snippet to achieve this:\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # Features\n",
    "y = iris.target  # Target labels\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a KNN classifier with weighted voting and a specified number of neighbors (k)\n",
    "k = 3\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=k, weights='distance')  # Use 'distance' for weighted voting\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = knn_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy on the test set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "```\n",
    "\n",
    "In this code:\n",
    "\n",
    "1. We load the Iris dataset and split it into features (X) and target labels (y).\n",
    "\n",
    "2. We split the dataset into training and testing sets using `train_test_split`, with 70% of the data for training and 30% for testing.\n",
    "\n",
    "3. We create a KNN classifier with `KNeighborsClassifier` and specify the number of neighbors (k) to use. Additionally, we set the `weights` parameter to 'distance' to enable weighted voting based on the inverse of distances.\n",
    "\n",
    "4. We fit the KNN classifier to the training data using the `fit` method.\n",
    "\n",
    "5. We make predictions on the test data using the `predict` method, and these predictions will take into account the distances to the neighbors.\n",
    "\n",
    "6. Finally, we calculate and print the accuracy of the KNN classifier with weighted voting on the test set to assess its performance.\n",
    "\n",
    "Weighted voting assigns more influence to closer neighbors in the decision-making process, which can lead to improved accuracy, especially when some neighbors are more relevant than others. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897f2672-7de9-409b-ad94-0ac96676980c",
   "metadata": {},
   "source": [
    "Q6. Implement a function to standardise the features before applying KNN classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36f6976-54c4-4836-8d0b-a137553d985b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "ANS:\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    To standardize the features before applying a K-Nearest Neighbors (KNN) classifier, you can create a function that uses the `StandardScaler` from scikit-learn to scale the features to have zero mean and unit variance. Here's a Python function to perform feature standardization:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def standardize_features(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Standardize features using StandardScaler.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train: Training data features (numpy array or pandas DataFrame).\n",
    "    - X_test: Test data features (numpy array or pandas DataFrame).\n",
    "\n",
    "    Returns:\n",
    "    - X_train_scaled: Standardized training data features.\n",
    "    - X_test_scaled: Standardized test data features.\n",
    "    \"\"\"\n",
    "    # Create a StandardScaler object\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit the scaler on the training data and transform both training and test data\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    return X_train_scaled, X_test_scaled\n",
    "```\n",
    "\n",
    "You can use this function by passing your training and test data features (X_train and X_test) as arguments. The function will return the standardized training and test data features, which you can then use for your KNN classifier or other machine learning algorithms.\n",
    "\n",
    "Here's how you can use the `standardize_features` function in your KNN classifier code:\n",
    "\n",
    "```python\n",
    "# Load necessary libraries and data, and split the dataset into X_train and X_test\n",
    "\n",
    "# Standardize features\n",
    "X_train_scaled, X_test_scaled = standardize_features(X_train, X_test)\n",
    "\n",
    "# Create a KNN classifier and fit it to the standardized training data\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=k)\n",
    "knn_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the standardized test data\n",
    "y_pred = knn_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy or other evaluation metrics on the test set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "```\n",
    "\n",
    "By standardizing the features before applying the KNN classifier, you ensure that all features have the same scale and zero mean, which helps the algorithm make more meaningful distance calculations and can improve the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b928e7f9-7e68-4cce-8fd5-ce835ae7fd3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
